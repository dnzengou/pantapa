{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dnzengou/pantapa/blob/master/pantapa_VisualizingData_v1.ipynb\" \n",
    "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Data for Classification\n",
    "\n",
    "In the previous lab, you explored the automotive price dataset to understand the relationships for a regression problem. In this lab you will explore the German bank credit dataset to understand the relationships for a **classification** problem. The difference being, that in classification problems the label is a categorical variable. \n",
    "\n",
    "In other labs you will use what you learn through visualization to create a solution that predicts the customers with bad credit. For now, the focus of this lab is on visually exploring the data to determine which features may be useful in predicting customer's bad credit.\n",
    "\n",
    "Visualization for classification problems shares much in common with visualization for regression problems. Colinear features should be identified so they can be eliminated or otherwise dealt with. However, for classification problems you are looking for features that help **separate the label categories**. Separation is achieved when there are distinctive feature values for each label category. Good separation results in low classification error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data set \n",
    "\n",
    "### Prepare data to a manageable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing bson files***\n",
    "\n",
    "source:\n",
    "- [Kaggle](https://www.kaggle.com/inversion/processing-bson-files?select=category_names.csv)\n",
    "- [Access and process nested objects, arrays or JSON](https://hackersandslackers.com/extract-data-from-complex-json-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unziping (file on linux)\n",
    "#unzip pantapa_api_development.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Convert bson files with, optionally, the outputs documents in a pretty-printed format JSON\n",
    "#bsondump --pretty --outFile collection.json collection.bson\n",
    "## OR via https://json-bson-converter.appspot.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vouchertypes.json', 'scans.json', 'brands.json', 'appinfos.json', 'voucherurls.json', 'vouchers.json', 'vouchertypeurls.json', 'organizations.json', 'materialtypes.json', 'companies.json', 'stations.json', 'prescans.json']\n"
     ]
    }
   ],
   "source": [
    "## List all the mongodb data .bson files in the dedicated folder\n",
    "\n",
    "import os\n",
    "\n",
    "json_arr = os.listdir('data/data-pantapa_bson2json')\n",
    "print(json_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appinfos.bson\n",
      "appinfos.metadata.json\n",
      "brands.bson\n",
      "brands.metadata.json\n",
      "codenotfounds.metadata.json\n",
      "companies.bson\n",
      "companies.metadata.json\n",
      "materialtypes.bson\n",
      "materialtypes.metadata.json\n",
      "modulehashes.metadata.json\n",
      "organizations.bson\n",
      "organizations.metadata.json\n",
      "packages.metadata.json\n",
      "prescans.bson\n",
      "prescans.metadata.json\n",
      "scans.bson\n",
      "scans.metadata.json\n",
      "sessiontokens.bson\n",
      "stations.bson\n",
      "stations.metadata.json\n",
      "tokens.bson\n",
      "tokens.metadata.json\n",
      "userinformations.metadata.json\n",
      "vouchers.bson\n",
      "vouchertypes.bson\n",
      "vouchertypes.metadata.json\n",
      "vouchertypeurls.bson\n",
      "voucherurls.bson\n",
      "voucherurls.metadata.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Alternatively, proceed as below\n",
    "## Eg. list all bson files Input data files contained in pantapa_api_development directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output(['ls', 'data/data-pantapa_bson']).decode('utf8'))\n",
    "\n",
    "# Any results writen to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert files from json to csv, for ease of processing and visualization\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': {'machine': -1768797184, 'inc': 299119782, 'time': 1576742726}, 'data': {'name': 'Test', 'active': True, 'alreadyConnected': True, 'show_popup_notification': False}, 'meta': {'timestamp': {'createdAt': 1576742726344, 'updatedAt': 1576742726344}}, 'local': {'sv': {'name': 'Test'}}, '__v': 0}\n"
     ]
    }
   ],
   "source": [
    "## Read and print JSON files into the directory in JSON format\n",
    "## Let's start with companies\n",
    "\n",
    "# Open the existing JSON file for loading into a variable\n",
    "with open('data/data-pantapa_bson2json/companies.json') as json_file:\n",
    "  companies = json.load(json_file) #This does the same as above, reading the json file and storing it into a variable (dict)\n",
    "\n",
    "print(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__v\": 0,\n",
      "    \"_id\": {\n",
      "        \"inc\": 299119782,\n",
      "        \"machine\": -1768797184,\n",
      "        \"time\": 1576742726\n",
      "    },\n",
      "    \"data\": {\n",
      "        \"active\": true,\n",
      "        \"alreadyConnected\": true,\n",
      "        \"name\": \"Test\",\n",
      "        \"show_popup_notification\": false\n",
      "    },\n",
      "    \"local\": {\n",
      "        \"sv\": {\n",
      "            \"name\": \"Test\"\n",
      "        }\n",
      "    },\n",
      "    \"meta\": {\n",
      "        \"timestamp\": {\n",
      "            \"createdAt\": 1576742726344,\n",
      "            \"updatedAt\": 1576742726344\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Or in pretty json\n",
    "print(json.dumps(companies, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Note. We obtain below the same result as when proceeding as above\n",
    "companies = pd.read_json('data/data-pantapa_bson2json/companies.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's convert companies into csv format. We will do the same for the other json files\n",
    "companies.to_csv (r'data/data-pantapa_json2csv/companies.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>data</th>\n",
       "      <th>meta</th>\n",
       "      <th>local</th>\n",
       "      <th>__v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'machine': -1768797184, 'inc': 299119782, 'ti...</td>\n",
       "      <td>{'name': 'Test', 'active': True, 'alreadyConne...</td>\n",
       "      <td>{'timestamp': {'createdAt': 1576742726344, 'up...</td>\n",
       "      <td>{'sv': {'name': 'Test'}}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id  \\\n",
       "0  {'machine': -1768797184, 'inc': 299119782, 'ti...   \n",
       "\n",
       "                                                data  \\\n",
       "0  {'name': 'Test', 'active': True, 'alreadyConne...   \n",
       "\n",
       "                                                meta  \\\n",
       "0  {'timestamp': {'createdAt': 1576742726344, 'up...   \n",
       "\n",
       "                      local  __v  \n",
       "0  {'sv': {'name': 'Test'}}    0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's check the structure of this newly converted csv file\n",
    "companies_csv = pd.read_csv('data/data-pantapa_json2csv/companies.csv')\n",
    "\n",
    "companies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data structure for a few of these objects and dictionaries (dict) shows that the csv files do not look like something we want to use for visualization (nested data)... We willl work on json format instead. An easier way could have been to load the bson files on MongoDB, then selecting data subsets of interest for further analysis; we will go straight to that step with the queries down below (in the processing section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's proceed with brands file: read json (already done above) and convert to csv\n",
    "#brands = pd.read_json('data/data-pantapa_bson2json/brands.json', lines=True)\n",
    "brands.to_csv (r'data/data-pantapa_json2csv/brands.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': {'machine': 1762676284, 'inc': 1074367085, 'time': 1560947259}, 'data': {'name': 'Apoteket AB', 'image': {'key': 'development/brands-image/5d0a2a3b69104e3c40098a6d-apoteket_logo_png', 'source': 'https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d-apoteket_logo_png'}, 'active': True, 'company_id': None, 'country_code': ['SE'], 'deep_link': 'https://app.pantapa.com/N3AKBENTKSpAgZeq9'}, 'meta': {'timestamp': {'createdAt': 1566215629675, 'updatedAt': 1591180045366}}, 'local': {'sv': {'order': 10, 'company_name': None, 'company_address': None, 'post_address': None, 'vat_nr': None, 'contact_person': {}, 'how_it_works': {'package_name': 'Apoteket ABs plastpåsar', 'image_link': {'key': 'development/brands-image/5d0a2a3b69104e3c40098a6d_package-Apoteket_Bags_Green_png', 'source': 'https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d_package-Apoteket_Bags_Green_png', '__typename': 'AwsImage'}, 'text_line1': 'See available stores', 'text_line1_url': 'https://www.apoteket.se/globalassets/om-apoteket/hallbar-utveckling/apotek-med-pantbara-pasar-2019_apoteket_se.pdf', 'text_line2': 'Upp till 2 SEK per pantad påse.', 'show_brand_logo': True, 'description': ['Plastpåsar sålda i utvalda buikter i Sverige.', 'Upp till 2 SEK per skannad plastpåse. ']}}, 'en': {'order': 10, 'company_name': None, 'company_address': None, 'post_address': None, 'vat_nr': None, 'contact_person': {}, 'how_it_works': {'package_name': \"Apoteket AB's plastic bags\", 'image_link': {'key': 'development/brands-image/5d0a2a3b69104e3c40098a6d_package-Apoteket_Bags_Green_png', 'source': 'https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d_package-Apoteket_Bags_Green_png', '__typename': 'AwsImage'}, 'text_line1': 'Se tillgängliga butiker', 'text_line1_url': 'https://www.apoteket.se/globalassets/om-apoteket/hallbar-utveckling/apotek-med-pantbara-pasar-2019_apoteket_se.pdf', 'text_line2': 'Up to SEK 2 per deposited bag.', 'show_brand_logo': True, 'description': ['Plastic bags sold in selected stores in Sweden.', 'Up to SEK 2 per scanned bag']}}}}\n"
     ]
    }
   ],
   "source": [
    "print(brands)\n",
    "#print(json.dumps(brands, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "materialtypes = pd.read_json('data/data-pantapa_bson2json/materialtypes.json', lines=True)\n",
    "materialtypes.to_csv (r'data/data-pantapa_json2csv/materialtypes.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations = pd.read_json('data/data-pantapa_bson2json/organizations.json', lines=True)\n",
    "organizations.to_csv (r'data/data-pantapa_json2csv/organizations.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescans = pd.read_json('data/data-pantapa_bson2json/prescans.json', lines=True)\n",
    "prescans.to_csv (r'data/data-pantapa_json2csv/prescans.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = pd.read_json('data/data-pantapa_bson2json/scans.json', lines=True)\n",
    "scans.to_csv (r'data/data-pantapa_json2csv/scans.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_json('data/data-pantapa_bson2json/stations.json', lines=True)\n",
    "stations.to_csv (r'data/data-pantapa_json2csv/stations.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vouchertypes = pd.read_json('data/data-pantapa_bson2json/vouchertypes.json', lines=True)\n",
    "vouchertypes.to_csv (r'data/data-pantapa_json2csv/vouchertypes.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vouchertypeurls = pd.read_json('data/data-pantapa_bson2json/vouchertypeurls.json', lines=True)\n",
    "vouchertypeurls.to_csv (r'data/data-pantapa_json2csv/vouchertypeurls.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "voucherurls = pd.read_json('data/data-pantapa_bson2json/voucherurls.json', lines=True)\n",
    "voucherurls.to_csv (r'data/data-pantapa_json2csv/voucherurls.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the list of converted csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brands.csv\n",
      "companies.csv\n",
      "materialtypes.csv\n",
      "organizations.csv\n",
      "prescans.csv\n",
      "scans.csv\n",
      "stations.csv\n",
      "vouchers.csv\n",
      "vouchertypes.csv\n",
      "vouchertypeurls.csv\n",
      "voucherurls.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output(['ls', 'data/data-pantapa_json2csv']).decode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract objects from nested JSON\n",
    "#### and explore datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.0373788, 59.3313148]\n"
     ]
    }
   ],
   "source": [
    "## Inspect content of the scans dictionary\n",
    "print(scans['data']['enums']['location']['coordinates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lat = 59.3313148, Long = 18.0373788\n",
    "![station0](img/Latitude-Longitude_Point0.png)\n",
    "[source](https://getlatlong.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apoteket stor grå påse\n"
     ]
    }
   ],
   "source": [
    "print(scans['data']['enums']['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed the same way with the other dictionaries obtained above from reading json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.5285912, 53.9204432]\n"
     ]
    }
   ],
   "source": [
    "print(prescans['data']['enums']['location']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENDING\n"
     ]
    }
   ],
   "source": [
    "print(prescans['data']['enums']['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brunchägg L 12-p inbur HP\n"
     ]
    }
   ],
   "source": [
    "print(prescans['data']['enums']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556777805897\n"
     ]
    }
   ],
   "source": [
    "print(vouchers['data']['redeem_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-14T00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(vouchers['data']['coupon']['validTo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panta Påsen Test\n"
     ]
    }
   ],
   "source": [
    "print(vouchers['data']['coupon']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://p.kupong.se/LY2Ujv64yF\n"
     ]
    }
   ],
   "source": [
    "print(vouchers['data']['coupon']['htmlLink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LY2Ujv64yF\n"
     ]
    }
   ],
   "source": [
    "print(vouchers['data']['coupon']['couponCode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there is only one brand in this file. Not enough to draw any pattern or trend, yet interersting to explore in depth some variables of interest for information purpose. To get to know the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apoteket AB\n"
     ]
    }
   ],
   "source": [
    "print(brands['data']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d-apoteket_logo_png\n"
     ]
    }
   ],
   "source": [
    "print(brands['data']['image']['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![source](https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d-apoteket_logo_png)\n",
    "[apoteket_logo](https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d-apoteket_logo_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SE']\n"
     ]
    }
   ],
   "source": [
    "print(brands['data']['country_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d_package-Apoteket_Bags_Green_png\n"
     ]
    }
   ],
   "source": [
    "print(brands['local']['sv']['how_it_works']['image_link']['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bags_green](https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d_package-Apoteket_Bags_Green_png)\n",
    "[Bags_Green](https://panta-pasen.s3.amazonaws.com/development/brands-image/5d0a2a3b69104e3c40098a6d_package-Apoteket_Bags_Green_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.apoteket.se/globalassets/om-apoteket/hallbar-utveckling/apotek-med-pantbara-pasar-2019_apoteket_se.pdf\n"
     ]
    }
   ],
   "source": [
    "print(brands['local']['sv']['how_it_works']['text_line1_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apoteket ABs plastpåsar\n"
     ]
    }
   ],
   "source": [
    "print(brands['local']['sv']['how_it_works']['package_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Plastic bags sold in selected stores in Sweden.', 'Up to SEK 2 per scanned bag']\n"
     ]
    }
   ],
   "source": [
    "print(brands['local']['en']['how_it_works']['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__v\": 0,\n",
      "    \"_id\": {\n",
      "        \"inc\": -2072099102,\n",
      "        \"machine\": 1962819352,\n",
      "        \"time\": 1545242828\n",
      "    },\n",
      "    \"data\": {\n",
      "        \"address\": \"Borgarfjordsgatan 8, 164 40 Kista, Sweden\",\n",
      "        \"country_code\": \"SE\",\n",
      "        \"description\": \"Laudantium et dignissimos voluptate eos. Dolorum quo voluptas corporis id aliquid magni voluptas. Soluta ducimus voluptas vel aut nihil ullam. Debitis consequatur vitae. Culpa voluptates tempora aut. Voluptatem occaecati voluptatem.\",\n",
      "        \"disabled\": false,\n",
      "        \"location\": {\n",
      "            \"coordinates\": [\n",
      "                17.9472797,\n",
      "                59.4067509\n",
      "            ],\n",
      "            \"type\": \"Point\"\n",
      "        },\n",
      "        \"name\": \"Test 1\",\n",
      "        \"point_type\": \"STATION\",\n",
      "        \"public\": true,\n",
      "        \"scan_distance\": 100,\n",
      "        \"store\": \"Eriksson - Nilsson\",\n",
      "        \"type_id\": []\n",
      "    },\n",
      "    \"meta\": {\n",
      "        \"created\": 1545242828280,\n",
      "        \"owner\": \"PP\",\n",
      "        \"timestamp\": {\n",
      "            \"updatedAt\": 1597910340625\n",
      "        },\n",
      "        \"updated\": 1545242828280\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## From the Read and print JSON file in JSON format previous steps,\n",
    "\n",
    "# Also equivalent to what obtained by the queries below (opening the existing JSON file for loading into a variable)\n",
    "#with open('data/data-pantapa_bson2json/stations.json') as json_file:\n",
    "#  stations = json.load(json_file)\n",
    "\n",
    "## Let's print pretty JSON data\n",
    "print(json.dumps(stations, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's look closer at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borgarfjordsgatan 8, 164 40 Kista, Sweden\n"
     ]
    }
   ],
   "source": [
    "print(stations['data']['address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.9472797, 59.4067509]\n"
     ]
    }
   ],
   "source": [
    "print(stations['data']['location']['point_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lat = 59.4067509, Long = 17.9472797\n",
    "![station1](img/Latitude-Longitude_Point1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATION\n"
     ]
    }
   ],
   "source": [
    "print(stations['data']['point_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(stations['data']['scan_distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eriksson - Nilsson\n"
     ]
    }
   ],
   "source": [
    "print(stations['data']['store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443409920\n"
     ]
    }
   ],
   "source": [
    "print(organizations['_id']['machine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ika\n"
     ]
    }
   ],
   "source": [
    "print(organizations['data']['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<code>As datasets do not have significant numbers of products to visualize, extract patterns and/or predict future behaviours (such as articles often bought, i.e scanned, together), we will do the predictive analytics work on a dummy dataset of choice. For this purpose, we will put ourselves in the situation where ALL products are scannable. The method of choice we will implement is call <b>Market Basket Analysis<b></code>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Basket Analysis (MBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first part, we will briefly explain the MBA basics and illustrate it with a case study of items scanned in a supermarket. In the second part we will implement this technique in python language programming using public [dataset](https://raw.githubusercontent.com/limchiahooi/market-basket-analysis/master/BreadBasket_DMS.csv) from model some source coded on github.<br>\n",
    "References at the end of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <a name=\"understanding-mba\">Understanding MBA</a> \n",
    " In this hypothetical case study, we are going to use the **Apriori algorithm** for frequent pattern mining to perform a Market Basket Analysis. Following sources ([Xavier Vivancos García](https://www.kaggle.com/xvivancos/market-basket-analysis)), \"MBA is a technique used by large retailers to *uncover associations between items*. It works by looking for combinations of items that occur together frequently in transactions, providing information to understand the purchase behavior. The outcome of this type of technique is, in simple terms, a set of rules that can be understood as “if this, then that”.\" \n",
    "\n",
    " Additional sources ([limchiahooi](https://github.com/limchiahooi/market-basket-analysis)), define \"Market basket analysis (MBA), also known as **association-rule mining**, as a method of discovering *customer purchasing patterns* by extracting *associations or co-occurrences* from stores' transactional databases.  It is a modelling technique based upon the theory that if you buy a certain group of items, you are more (or less) likely to buy another group of items. For example, if you are in a supermarket and you buy a loaf of Bread, you are more likely to buy a packet of Butter at the same time than somebody who didn't buy the Bread. (...)\" <br>\n",
    " Same principle can in theory be applied to *scanned items* -- as the scanning process is an integrated part of the purchasing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Applications ###\n",
    "There are many real-life applications of MBA:\n",
    "- **Recommendation engine** – showing related products as \"Customers Who Bought This Item Also Bought\" or “Frequently bought together” (as shown in the Amazon example above). It can also be applied to recommend videos and news article by analyzing the videos or news articles that are often watched or read together in a user session.\n",
    "<br>\n",
    "<br>\n",
    "- **Cross-sell / bundle products** – selling associated products as a \"bundle\" instead of individual items. For example, transaction data may show that customers often buy a new phone with screen protector together. Phone retailers can then package new phone with high-margin screen protector together and sell them as a bundle, thereby increasing their sales.\n",
    "<br>\n",
    "<br>\n",
    "- **Arrangement of items in retail stores** – associated items can be placed closer to each other, thereby invoking \"impulse buying\". For example it may be uncovered that customers who buy Barbie dolls also buy candy at the same time. Thus retailers can place high-margin candy near Barbie doll display, thereby tempting customers to buy them together.\n",
    "<br>\n",
    "<br>\n",
    "Etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Case Study ###\n",
    "We are analyzing the hypothetic scanning case of two items – Bread and Butter. We want to know if there is any evidence that suggests that scanning Bread leads to scanning Butter. Note. We will often replace scanning by transaction, interchangeably.\n",
    "\n",
    "**Problem Statment:** Is the pscanning of Bread leads to the scanning of Butter?<br><br>\n",
    "**Hypothesis:** There is significant evidence to show that scanning Bread leads to scanning Butter. (As much as buying Bread leads to buying Butter)\n",
    "\n",
    "\n",
    "Bread => Butter\n",
    "\n",
    "Antecedent => Consequent\n",
    "\n",
    "Let's consider a supermarket which generates **1,000 transactions monthly**, of which **Bread was purchased in 150 transactions, Butter in 130 transactions, and both together in 50 transactions**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Findings ###\n",
    "We can use MBA to extract the association rule between Bread and Butter. There are *three metrics* or criteria to evaluate the strength or quality of an association rule, which are **support**, **confidence** and **lift**. (*Convictions* is an additional metric used in some cases)<br>\n",
    "More about this [here](https://medium.com/datadriveninvestor/product-recommendation-using-association-rule-mining-for-a-grocery-store-7e7feb6cd0f9)\n",
    "\n",
    "In short,\n",
    "- Support measures the percentage of transactions containing a particular combination of items relative to the total number of transactions. <br>In our example: *Support (antecedent (Bread) and consequent (Butter)) = Number of transactions having both items / Total transactions*. <br>Result: **The support value of 5% means 5% of all transactions have this combination of Bread and Butter scanned together**. Since the value is above the threshold of 1%, it shows there is indeed **support** for this association and thus *satisfy the first criteria*.\n",
    "![alt text](img/support.jpg \"Support\")\n",
    "\n",
    "- Confidence measures the probability of finding a particular combination of items whenever antecedent is bought. <br> *Confidence (antecedent i.e. Bread and consequent i.e. Butter) = P (Consequent (Butter) is bought GIVEN antecedent (Bread) is bought)*. <br> Result: **The confidence value of 33.3% is above the threshold of 25%**, indicating we can be **confident** that Butter will be scanned whenever Bread is scanned, and thus *satisfy the second criteria*.\n",
    "![alt text](img/confidence.jpg \"Confidence\")\n",
    "\n",
    "- Lift is a metric to determine how much the transaction between antecedent and consequent influence each other. <br>We want to know which is higher, P(Butter) or P(Butter / Bread)? (Conditional probabilities) If the scanning of Butter is influenced by the one of Bread, then the *ratio of P(Butter / Bread) over P(Butter) > 1*.<br> Result: **The lift value of 2.56 is greater than 1**, thus that the transaction for Butter is indeed **influenced** by the one for Bread which *satisfy the third criteria*. This also means that Bread's transaction lifts the Butter's purchase by 2.56 times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Takeaways ###\n",
    "Based on the findings above, we\n",
    "\n",
    "    a) Have the support of 5% transactions for Bread and Butter in the same basket\n",
    "    b) Have 33.3% confidence that Butter scan happen whenever Bread is scanned.\n",
    "    c) Know the lift in Butter's transaction is 2.56 times more whenever Bread is involved than when Butter is alone.\n",
    "\n",
    "Therefore, we can justify our initial hypothesis by concluding that there is indeed evidence to suggest that the *transaction for Bread leads to the one for Butter*. This is a valuable insight to guide decision-making.\n",
    "<br>Actions forward could be, among other things, for retail stores to start placing bread and butter close to each other, knowing that customers are highly likely to \"impulsively\" scanned (and ultimately purchase) them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <a name=\"implementation-in-python\">Implementation in Python</a> ##\n",
    "On a large dataset, leveraging on Python libraries for a ready-made algorithm is more efficient than the use of traditional Ms Excel to calculate support, confidence and lifts. Furthermore, as the popular scikit-learn library does not allow us to apply *Apriori algorithm* for extracting frequent item sets for further analysis, because not supported this algorithm, we use another library instead: [MLxtend (machine learning extensions)](http://rasbt.github.io/mlxtend/) by Sebastian Raschka. [Chris Moffitt](http://pbpython.com/market-basket-analysis.html) also provides a tutorial on using MLxtend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note. If you are using Jupyter Notebook, the MLxtend library does not come pre-installed with Anaconda (which I am using right now). You can easily install this package with conda by running one of the following in your Anaconda Prompt:<br><br>\n",
    "`conda install -c conda-forge mlxtend`<br>\n",
    "`conda install -c conda-forge/label/gcc7 mlxtend`<br><br>\n",
    "Or with pip:<br><br>\n",
    "`!pip install mlxtend`<br>(\"!\" if cell ran from the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The [dataset](https://github.com/dnzengou/pantapa/data/MBA/MBA.csv) we are using in the case study in this is inspired from a publicly available one initially from Kaggle, now hosted on [github](https://github.com/limchiahooi/market-basket-analysis/blob/master/BreadBasket_DMS.csv) which contains the Transactions data from a bakery from 30/10/2016 to 09/04/2017. The original data belongs to a real bakery called \"The Bread Basket\" that serves coffee, bread, muffin, cookies etc. located in the historic center of Edinburgh.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries required\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into a pandas dataframe and take a look at the first 10 rows\n",
    "bread = pd.read_csv(\"https://raw.githubusercontent.com/limchiahooi/market-basket-analysis/master/BreadBasket_DMS.csv\")\n",
    "bread.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori algorithm\n",
    "#### for frequent pattern mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information in these plots. The key to interpreting these plots is comparing the proportion of the categories for each of the label values. If these proportions are distinctly different for each label category, the feature is likely to be useful in separating the label.  \n",
    "\n",
    "There are several cases evident in these plots:\n",
    "1. Some features such as checking_account_status and credit_history have significantly different distribution of categories between the label categories. \n",
    "2. Others features such as gender_status and telephone show small differences, but these differences are unlikely to be significant. \n",
    "3. Other features like other_signators, foreign_worker, home_ownership, and job_category have a dominant category with very few cases of other categories. These features will likely have very little power to separate the cases.  \n",
    "\n",
    "Notice that only a few of these categorical features will be useful in separating the cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have performed exploration and visualization to understand the relationships in a classification dataset. Specifically:\n",
    "1. Examine the imbalance in the label cases using a frequency table. \n",
    "2. Find numeric or categorical features that separate the cases using visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### References ###\n",
    "- Amir, A. (2019, February 3). Association Rule(Apriori and Eclat Algorithms) with Practical Implementation. *Medium*. Retrieved from https://medium.com/machine-learning-researcher/association-rule-apriori-and-eclat-algorithm-4e963fa972a4\n",
    "- Kaushik, D. (2019, January 15). Product Recommendation Case Study Using Apriori Algorithm for a Grocery Store. *Medium*. Retrieved from https://medium.com/datadriveninvestor/product-recommendation-using-association-rule-mining-for-a-grocery-store-7e7feb6cd0f9\n",
    "- Madalina, C. (2019, Juin 8). An introduction to frequent pattern mining research. Summary of Apriori, Eclat and FP tree algorithms. *IMedium*. Retrieved from https://medium.com/@ciortanmadalina/an-introduction-to-frequent-pattern-mining-research-564f239548e\n",
    "- Andrewngai (2020, March 17). Understand and Build FP-Growth Algorithm in Python. Frequency Pattern Mining using FP-tree and conditional FP-tree in Python. *Towards Data Science*. Retrieved from https://towardsdatascience.com/understand-and-build-fp-growth-algorithm-in-python-d8b989bab342\n",
    "- Xavier Vivancos, G. (2020, May). Market Basket Analysis. *Kaggle*. Retrieved from https://www.kaggle.com/xvivancos/market-basket-analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
